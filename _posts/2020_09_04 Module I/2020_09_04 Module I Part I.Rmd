---
title: "Cognitive Psychology Foundations - Module I Part I"
description: |
  I explore some foundational work investigating cognitive psychology, it's origins, and different ways in which the field has been studied.
author:
  - name: Lyndsay Hage
    url: https://lyndsaymh.github.io/Lyndsay-s-Cognitive-Psychology-Blog/
    affiliation: Hunter College - City University of New York
    affiliation_url: https://www.cuny.edu
date: 09-04-2020
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Learn more about creating blogs with Distill at:
# https://rstudio.github.io/distill/blog.html

```

This blog will be a series of notes and thoughts as I process the assigned readings from my Cognitive Psychology course regarding the foundations of the field. Sensation, perception, and computational methods are not my strongest topics, but I thoroughly enjoyed learning as I read.

In Miller’s (1956) article, he describes how we can view information measurement through the study of absolute judgment or the capacity someone has to transmit information. How can we quantify how much information a person can obtain (input) and communicate (output)? I have rarely viewed information acquisition and communication in such a mechanistic and quantitative way. It took me a while to feel comfortable with his claims, but once I began to get the hang of his descriptions (and when I made some great Venn diagrams based on his writing) it began to make sense. Doodling always tends to set me right. Miller describes how an observer is given an amount of information in the stimulus presented to them and the observer must be able to transmit this information through their responses. If we increase the amount of information we provide to an observer, the transmitted information will increase at first as well but then it will eventually level off. It makes sense that the more information we input to an observer, the more likely it will be that the observer will make errors. What is happening when an observer’s response levels off or looks graphically like an asymptote? That is the observer’s channel capacity. To test an observer’s absolute judgment, we look at their channel capacity or the most amount of information that an observer can output that matches accurately with the stimuli given (input). 

Miller describes how humans seem to be limited in our channel capacities. For example, on stimuli that varies across one dimension (in his first example: pitch), the amount of transmitted information increases linearly up to about 2 bits and then bends off into an asymptotic value at around 2.5 bits reflecting a channel capacity of the observer for their absolute judgment on pitch. My understanding is that this means most people tested can only identify one out of five or six pitches before they begin to get confused. We can only give them about six different classes or variations on a single dimension and consistently get them to choose without error. He goes on to describe how these capacities vary across all sorts of stimuli that vary across a single dimension and finds that there is a considerable similarity among these single dimension variables. Humans seem to have a finite and small capacity to make these absolute judgements with channel capacities staying in a range of 2 to 15 categories with a mean of 6.5 categories (around 7 – I can start to see it now). 

Humans appear to do better with multi-dimensional stimuli that change across two or more variables.  When we add more variables to the display, we increase our channel capacity, but we are not particularly accurate for any particular variable alone. I started to piece this together when Miller explains this in terms of evolutionary theory. It makes that animals did well making judgements based on small amounts of information on a lot of environmental stimuli than to have a large amount of information on a small segment of the environment. I thought of comparing two monkeys: One monkey can determine that a leopard-shaped creature is nearby and is moving closer, that there is an eagle-shaped bird that is flying far away to the North, and that there is a tree nearby. With all this information it can then quickly respond by jumping into that tree for cover.  At the same time, another monkey meticulously and accurately determines how many feathers the eagle has and then it must respond to the information it has available (perhaps attempting to escape the eagle but not the nearby leopard). The first monkey is far more likely to survive the leopard attack than the second, because it took in a little bit of information about a lot of things (maybe some of which was inaccurate) to survive in a world full of fluctuating stimuli. The monkey doesn’t need to attend to the feathers of an eagle if there is a leopard close by and a tree that will be the best mode of escape. Survival doesn’t need accuracy on a particular variable in the environment. Now, if the task in question, instead of survival, was to ask both monkeys how many feathers the eagle had, the first monkey would not likely report accurately on that particular variable whereas the second monkey would be far more likely to achieve accuracy on this variable.  Good thing survival doesn’t (usually) depend on our feather counting abilities. 

I like how Miller expands this to language and how human speech utilizes 8 – 10 distinctive features that distinguish one phoneme from another and each language is limited to about 8 or 9 of these features. In order to increase our channel capacity, language uses phonemes, or varied combinations of these distinctive features to add complexity. We then combine these phonemes to make words and combine these words to make sentences. This increased complexity is also paired with the nature of langue: it goes in a sequence. As the sequence of features, phonemes, and words continues, it can provide us with context so that we can increase our accuracy. 

There are also limitations to the amount of information an observer can hold in their immediate memory. These limitations are focused on the number of items rather than the amount of information. When asking someone (let’s call her Sally) to recall 5 random words, Sally will not be limited to remember the distinctive features of these words or the distinctive phonemes but rather the 5 distinctive words (or chunks). Sally isn’t attending to the distinctive features which make up phonemes. She previously learned the various combinations of phonemes needed to make up words. She no longer needs to attend to the few bits because she has recoded sequences of phonemes over time into different words. 

Miller’s article about the number seven reminded me of the golden ratio. The ratio has been studied by mathematicians, artists, architects, musicians, and even biologists and some argue that the ratio appears across all these different mediums because it is somehow innately pleasing or holds some sort of natural truth. Historically, some have even used this ratio to argue that this ratio acts as a proof of the divine. Humans like patterns and symmetry. It feels nice. It feels comfortable. Whether it exemplifies some sort of natural truth is yet to be determined. Sometimes I worry that the reliance on mechanism and pattern can blind us to a world that is, in reality, far more complex than the patterns we are focusing on. How do we strike a balance between developing input-output models that mechanize the human experience and a self-reported and subjective Rorschach test? 

Miller, G. A. (1956). The magical number seven, plus or minus two: Some limits on our capacity for processing information. The Psychological Review, 63 (2). 

--------------------------------------------------------------------------------

In Newell’s (1973) article, he critiques the field of experimental psychology and how scientists in the field produce studies that exist within their own silos of investigation. Every scientist is focused on their models of information processing without necessarily investigating what role they play in a bigger picture. The publications he refers to throughout are considered well-designed and worthy of merit, but they all reflect the field’s struggle to contribute as a whole toward the universal truth they claim to be investigating. How does X model about information processing inform us about the nature of human information processing and how can we know it is reflective of nature? How does the model compare with or act among other models? Newell discusses how experimental psychology has been driven to its current mode of investigation and style by the focus on empirical exploration of phenomena and on binary oppositions. By focusing more on investigating single, defined phenomena and not investigating how each phenomenon plays a role in day to day decision making or how it influences or is impacted by other phenomena, these studies isolate themselves. When scientists isolate themselves to one theory, Newell suggests that we may not come closer to discovering absolute truth but rather farther away from clarity. The focus of experimental psychology on opposing binaries also resonated with me. Life is not usually coded in distinct binaries and by developing research that focuses on supporting or refuting a binary, this research will oversimplify the natural processes is attempting to investigate. 

Newell offers some possible suggestions for these problems. Some of these examples he gave did not make complete sense to me as he refers to studies and figures I do not have access to. Methodologically he proposes conducting research in which you know the method your subject will use on a task and, from this knowledge, you can then focus solely on the variation of what the subject actually does and whether your models are able to predict this.  He also asserts that one should never average over the methods used by subjects as it oversimplifies results and demonstrates regularity where they may not be any. Psychology is described as a field of study that is looking for the fixed structures that exist in nature so that, with this information, one can infer or predict the method a subject might use to solve a task in this context. He offers three possible methods to use when conducting experiments: constructing complete processing models with control structures in order to investigating a subject’s varying methods on a task, analyzing a complex task experimentally and theoretically to determine if one can predict a behavior, and developing a single program (such as an information processing model) for many tasks and see how it compares. 

Again, simple is nice. I understand the desire to silo off into an area of study that focuses on one small phenomenon and/or use one’s research to refute or deny some binary opposite related to that phenomena. This science feels clean and focused, but if we divvy up the world into clean, man-made categorizations, we might miss out on complex phenomena that (while probably messy) will help to explain how a human processes information or how a dog performs on a sequence of tasks.  

Newell, A. (1973). You can’t play 20 questions with nature and win: projective comments on the papers of this symposium. Visual Information Processing. 

--------------------------------------------------------------------------------

Schacter et al. (1978) write to describe Richard Semon, an early pioneer in cognitive science who first wrote about his theory of memory of in the early 20th century and whose work was considered far ahead of its time. This previously unknown work appears to anticipate recent (1970’s) developments in memory research. While other contemporaries of Semon’s focused on memory repetition effects, forgetting curves, memory acquisition, retention, and reproduction, Semon’s work covered many areas of memory including memory storage (called the Law of Engram), memory retrieval (called the Law of Ecphory), and many others. His analysis of memory retrieval was one of his principle theoretical concerns and this focus on the conditions, functions, and processes of retrieval was one of a few systematic attempts to investigate the role retrieval plays in memory at this time. He also analyzed sensation and perception and built his analysis of memory during a time when these processes were commonly reviewed as separate. Semon focused on fields or complexes of sensation rather than small sensation-elements which demonstrates great similarity to Gestalt analyses of perception. Semon’s theory emphasized the conditions, functions, and processes of retrieval in a systematic theoretical way.

Schacter et al. (1978) describe how different trends in psychology, such as introspective analysis, associative learning, and psychoanalysis influenced how memory was viewed at the time and how these contexts influenced what was considered important in describing memory. This is always important to keep in mind – science is not immune to cultural whims and popular trends. There was an obvious lack of interest in, or concern for, how memories are retrieved from 1885-1935. Several thinkers in the early part of the 20th century did not believe that memories were represented in the brain but were rather nonphysical or psychical form. Semon, who had a strong background in biology, wrote that engrams were stored through a physiochemical process in the brain and even debated whether memory was stored locally or distributed throughout the brain. He was criticized from all sides for his controversial views. 

Schachter et al. (1978) described Semon’s lack of recognition in the past and today as being the result of many things such as the lack of work investigating retrieval systematically, his use of invented terminology, his Lamarckian views around heredity and memory, and his lack of original experimental evidence to support his theory. Had he not fallen into obscurity, perhaps we would be much farther along memory research, specifically retrieval research, might be today. I often wonder about the scientists who fall to obscurity. If one’s work, even if it is exceptional, does not get properly promoted or a bias exists against a scientific theory or discovery it may go unnoticed and unexplored. 

Schacter, D. J., Eich, J. E., & Tulving, E. (1978). Richard Semon’s Theory of Memory. Journal of Verbal Learning and Verbal Behavior, 17, 721-743. 
